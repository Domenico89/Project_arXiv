{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that illustrates how to harvest data these codes are partly a readaptation of the ones made by Amdrej Karpathy in his brilliant project https://github.com/karpathy/arxiv-sanity-preserver.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import shutil\n",
    "from  urllib.request import urlopen\n",
    "import urllib.request\n",
    "import feedparser\n",
    "import random\n",
    "\n",
    "from utils import Config, get_id_version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following piece of code is essentially the same as the one contained in the module arXivAPI.py . The code connects with the arXiv search engine through queries and download the article metadata. The data is then saved in a binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error loading existing database:  [Errno 2] No such file or directory: 'data/metadata_db'\n",
      "Starting from an empty one\n",
      "Added 10 papers in category astro-ph.CO . Papers skipped 3\n",
      "Total number of papers added 10.\n"
     ]
    }
   ],
   "source": [
    "search_cat='astro-ph.CO' #Category of papers to be searched by the arXiv API. See http://arxiv.org/help/api/user-manual#detailed_examples\n",
    "#Example with \"Cosmology and Nongalactic Astrophysics\". If search_cat=None the module will harvest data relative to all the  categories. \n",
    "start_index=0 #0 = most recent API result\n",
    "max_index=10 #Upper bound on paper index we will fetch for each category.\n",
    "results_per_iteration=100 #Batch of results to be provided by the arXiv API.\n",
    "wait_time=3.0 #Waiting time between iterations to avoid being cut out by arXiv API.\n",
    "\n",
    "try:\n",
    "    with open(Config.metadata_db, 'rb') as file:\n",
    "        metadata_db = pickle.load(file)\n",
    "except Exception as e:\n",
    "    print('error loading existing database: ',e)\n",
    "    print('Starting from an empty one')\n",
    "    metadata_db = {}\n",
    "\n",
    "base_url='http://export.arxiv.org/api/query?'\n",
    "    \n",
    "if search_cat is None:\n",
    "    with open('arXiv_categories','rb') as file:\n",
    "        arXiv_categories=(pickle.load(file)).values()\n",
    "else:\n",
    "    arXiv_categories=[search_cat]\n",
    "        \n",
    "    \n",
    "num_added_tot=0\n",
    "for cat in arXiv_categories:\n",
    "    search_query='cat:'+cat\n",
    "    \n",
    "    num_cat_added_tot=0\n",
    "    i=0\n",
    "    while num_cat_added_tot<max_index:\n",
    "        query = 'search_query=%s&sortBy=lastUpdatedDate&start=%i&max_results=%i' % (search_query,i, results_per_iteration)\n",
    "   \n",
    "        with urllib.request.urlopen(base_url+query) as url:\n",
    "            response = url.read()\n",
    "        parse = feedparser.parse(response)\n",
    "        if len(parse.entries)==0:\n",
    "            print('There are no more search results for category %s .'%(cat))\n",
    "            break\n",
    "            \n",
    "        num_cat_added=0\n",
    "        num_cat_old=0\n",
    "\n",
    "        for e in parse.entries:\n",
    "            idx,v=get_id_version(e['id'])\n",
    "            e['raw_id']=cat+'/'+idx\n",
    "            e['version']=v\n",
    "        #add the article to the database only if the article is not there already (keeping the version into consideration) \n",
    "        #and if the primary category of the search is the same as the one considered\n",
    "            if e['arxiv_primary_category']['term']==cat and (not idx in metadata_db or v > metadata_db[idx]['version']):\n",
    "                metadata_db[idx]=e\n",
    "                num_cat_added+=1\n",
    "                num_cat_added_tot+=1\n",
    "            else:\n",
    "                num_cat_old+=1\n",
    "            if num_cat_added_tot>max_index-1:\n",
    "                break\n",
    "    \n",
    "        print('Added %i papers in category %s . Papers skipped %i'%(num_cat_added,cat,num_cat_old))\n",
    "        i+=results_per_iteration\n",
    "        num_added_tot=num_added_tot+num_cat_added\n",
    "        \n",
    "        #Waiting some seconds to avoid being cut out from arXiv\n",
    "        time.sleep(wait_time+random.uniform(0,0.1))\n",
    "\n",
    "        \n",
    "print('Total number of papers added %i.'%(num_added_tot))\n",
    "\n",
    "if num_added_tot > 0:\n",
    "    with open(Config.metadata_db, 'wb') as file:\n",
    "        pickle.dump(metadata_db,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following piece of code is essentially the same as the codes contained in the modules download_articles.py and articles_to_txt.py . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting article http://export.arxiv.org/pdf/0806.0664v6.pdf\n",
      "Getting article http://export.arxiv.org/pdf/astro-ph/9911331v2.pdf\n",
      "Getting article http://export.arxiv.org/pdf/astro-ph/0311033v5.pdf\n",
      "Getting article http://export.arxiv.org/pdf/astro-ph/0010594v3.pdf\n",
      "Getting article http://export.arxiv.org/pdf/0802.2889v4.pdf\n",
      "Getting article http://export.arxiv.org/pdf/astro-ph/0312617v2.pdf\n",
      "Getting article http://export.arxiv.org/pdf/astro-ph/0008166v8.pdf\n",
      "Getting article http://export.arxiv.org/pdf/0709.2329v2.pdf\n",
      "Getting article http://export.arxiv.org/pdf/0812.3401v2.pdf\n",
      "Getting article http://export.arxiv.org/pdf/astro-ph/0410297v2.pdf\n",
      "2004.01139 already exists, skipping.\n",
      "2004.01135 already exists, skipping.\n",
      "1911.08512 already exists, skipping.\n",
      "2004.00947 already exists, skipping.\n",
      "2004.00864 already exists, skipping.\n",
      "2004.00863 already exists, skipping.\n",
      "1909.13832 already exists, skipping.\n",
      "2004.00678 already exists, skipping.\n",
      "2004.00672 already exists, skipping.\n",
      "2004.00649 already exists, skipping.\n",
      "Downloaded 10 articles out of 10.\n"
     ]
    }
   ],
   "source": [
    "with open(Config.metadata_db,'rb') as file:\n",
    "    metadata_db=pickle.load(file)\n",
    "\n",
    "if not os.path.exists(Config.tmp): #create directory to temporarily store pdfs if not present aready\n",
    "    os.makedirs(Config.tmp)\n",
    "\n",
    "if not os.path.exists(Config.txt_db): #create directory to temporarily store pdfs if not present aready\n",
    "    os.makedirs(Config.txt_db)\n",
    "\n",
    "timeout=10 #waiting seconds before stopping the download\n",
    "already_have = set(os.listdir('txt_db')) #getting list of papers that are already present in the directory  \n",
    "\n",
    "num_to_add=0\n",
    "num_added=0\n",
    "with open(Config.metadata_db,'rb') as file:\n",
    "    metadata_db=pickle.load(file)\n",
    "\n",
    "for arXiv_id,metadata in metadata_db.items():\n",
    "    pdf=arXiv_id+'.pdf'\n",
    "    txt=arXiv_id+'.txt'\n",
    "    #getting the link of the pdf from the metadata, this is positioned at the end of the list at position 'links'\n",
    "    pdf_url=metadata['links'][-1]['href']+'.pdf'\n",
    "    #make the link into the link specifically provided by arXiv for harvesting purposes \n",
    "    pdf_url=pdf_url.replace(\"arxiv.org\", \"export.arxiv.org\")\n",
    "    pdf_path=os.path.join(Config.tmp,pdf)\n",
    "    txt_path=os.path.join(Config.txt_db,txt)\n",
    "    try:\n",
    "        if not txt in already_have:\n",
    "            num_to_add+=1\n",
    "            req = urlopen(pdf_url, None, timeout)\n",
    "            print('Getting article %s' % (pdf_url))\n",
    "            with open(pdf_path, 'wb') as file:\n",
    "                shutil.copyfileobj(req, file)\n",
    "            #converting the pdf into txt needs pdftotext on the system to run\n",
    "            cmd = \"pdftotext %s %s\" % (pdf_path, txt_path)\n",
    "            exit=os.system(cmd)\n",
    "            #remove the pdf to save space\n",
    "            os.system('rm %s'%(pdf_path))\n",
    "            num_added+=1\n",
    "            #check that everything went well\n",
    "            if exit!=0:\n",
    "                print('It seems like there was an error in converting %s. Please try again later. Exit status %i.'%(pdf,exit))\n",
    "                #remove the article in case the file was created\n",
    "                if os.path.isfile(txt_path):\n",
    "                    os.system('rm '+txt_path)\n",
    "                num_added-=1\n",
    "            \n",
    "        else:\n",
    "            print('%s already exists, skipping.' % (arXiv_id))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('An error incurred while downloading: %s .'%(pdf_url))\n",
    "        print(e)\n",
    "print('Downloaded %i articles out of %i.'%(num_added,num_to_add))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
