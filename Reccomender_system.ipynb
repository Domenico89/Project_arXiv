{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reccomender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data base in vectorised form we can also provide a way to provide articles that are similar to the one that was given as an input. This can be useful for authors at it might point out articles that are related with their work and that they were not aware of. As a first attempt to solve this type of problem we can use cosine similarity.\n",
    "\n",
    "The code contained here is basically the same contained in the module model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import shutil\n",
    "from  urllib.request import urlopen\n",
    "\n",
    "from utils import Config, read_clean, strip_extension\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Config.tfidf,'rb') as file:\n",
    "    tfidf=pickle.load(file)\n",
    "with open(Config.vectorized_articles,'rb') as file:\n",
    "    vectorized_articles=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vectorized_articles['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that return the articles in the database X that are closer to the given article x based on cos similarity\n",
    "def find_similar(X,x,how_many):\n",
    "    cos_similarity=x.dot(X.transpose())\n",
    "    cos_similarity=np.asarray(cos_similarity.todense())\n",
    "    return np.argsort(-cos_similarity)[:,:how_many]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an example on how this works on the first two articles of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2003.01771' '1910.00583' '1902.08464' '1807.09515' '1901.07357'\n",
      "  '2002.06177' '0910.5001' '1904.01053' '2002.05193' '1903.08348']\n",
      " ['1908.02651' '2001.01266' '2002.08316' '1908.02640' '1902.06655'\n",
      "  '1907.00271' '1906.09380' '2002.08161' '1906.00478' '1911.08356']]\n",
      "[['http://arxiv.org/pdf/2003.01771v1' 'http://arxiv.org/pdf/1910.00583v2'\n",
      "  'http://arxiv.org/pdf/1902.08464v3' 'http://arxiv.org/pdf/1807.09515v1'\n",
      "  'http://arxiv.org/pdf/1901.07357v1' 'http://arxiv.org/pdf/2002.06177v3'\n",
      "  'http://arxiv.org/pdf/0910.5001v1' 'http://arxiv.org/pdf/1904.01053v1'\n",
      "  'http://arxiv.org/pdf/2002.05193v2' 'http://arxiv.org/pdf/1903.08348v2']\n",
      " ['http://arxiv.org/pdf/1908.02651v3' 'http://arxiv.org/pdf/2001.01266v2'\n",
      "  'http://arxiv.org/pdf/2002.08316v1' 'http://arxiv.org/pdf/1908.02640v1'\n",
      "  'http://arxiv.org/pdf/1902.06655v1' 'http://arxiv.org/pdf/1907.00271v1'\n",
      "  'http://arxiv.org/pdf/1906.09380v1' 'http://arxiv.org/pdf/2002.08161v1'\n",
      "  'http://arxiv.org/pdf/1906.00478v3' 'http://arxiv.org/pdf/1911.08356v1']]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_articles['articles'][find_similar(X,X[0:2,:],10)])\n",
    "print(vectorized_articles['links'][find_similar(X,X[0:2,:],10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the reccomender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking the articles given by the reccomender system it can be seen that they seem to be very similar to the one given as an input. Anyhow we would like to have a more quantitative measurement on how good our reccomender system is. \n",
    "\n",
    "In general this is not an easy problem, as we do not know if the reccomended articles are relevant or not, as it would require to read the articles, which for large number of articles is clearly not feasible. One possible approach is then to consider as relevant all the articles that are cited by the paper that we are considering. Since it might be possible that not all the articles cited by artices in the database are also part of the database we will need to download the cited articles\n",
    "\n",
    "Unfortunately most of the time cited articles are not on free acess and this would render it difficult to download them. For this reason we will only consider citations to arXiv articles, which we can easily download. The following approach, therefore, present evident limitations, which could be overcame or at least reduced with more accessible resources. \n",
    "\n",
    "Another point to be consider is that sometimes not all the articles that are being cited are relevant, these type of complications could be possibly be resolved through some data labeling, wich we will not perform here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's therefore start by selecting a set of articles on which we will perform the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Config.metadata_db,'rb') as file:\n",
    "    metadata_db=pickle.load(file)\n",
    "\n",
    "txt_labels_train=[]\n",
    "\n",
    "for article in metadata_db.keys():\n",
    "    article_path=os.path.join(Config.txt_db,article+'.txt')\n",
    "    if os.path.isfile(article_path):\n",
    "        txt_labels_train.append(article)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = os.listdir(Config.txt_db)\n",
    "idxs=[strip_extension(txt) for txt in txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_db={}\n",
    "for idx in txt_labels_train:\n",
    "    with open(os.path.join(Config.txt_db,idx+'.txt'),'r') as file:\n",
    "        article=file.read()\n",
    "    citations=re.findall('arXiv:[0-9]+.[0-9]+|arXiv:[a-zA-Z-]+.[0-9]+|arXiv:[a-zA-Z-]+.[0-9]+.[0-9]',article)\n",
    "    if len(citations)>=10:#We restrict to those papers that have a sufficent number of citations\n",
    "        citations_db[idx]=citations[1:] # the first citation is generally the paper itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1993"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(citations_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see what the citations look like and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0807.4730': ['arXiv:astro-ph/0701517',\n",
       "  'arXiv:astro-ph/0101231',\n",
       "  'arXiv:astro-ph/0206286',\n",
       "  'arXiv:astro-ph/9807150',\n",
       "  'arXiv:astro-ph/0406254',\n",
       "  'arXiv:astro-ph/0701856',\n",
       "  'arXiv:astro-ph/0504149',\n",
       "  'arXiv:astro-ph/0109223',\n",
       "  'arXiv:0705.1972',\n",
       "  'arXiv:astro-ph/0201345',\n",
       "  'arXiv:astro-ph/0402180',\n",
       "  'arXiv:astro-ph/0512456',\n",
       "  'arXiv:astro-ph/0206336',\n",
       "  'arXiv:astro-ph/0106359',\n",
       "  'arXiv:astro-ph/0405275',\n",
       "  'arXiv:astro-ph/0702532',\n",
       "  'arXiv:astro-ph/0410214',\n",
       "  'arXiv:0707.4415',\n",
       "  'arXiv:astro-ph/0106502',\n",
       "  'arXiv:astro-ph/9902012',\n",
       "  'arXiv:astro-ph/0103150',\n",
       "  'arXiv:astro-ph/0106567',\n",
       "  'arXiv:astro-ph/0002481',\n",
       "  'arXiv:astro-ph/9906426',\n",
       "  'arXiv:astro-ph/0010381',\n",
       "  'arXiv:astro-ph/0103513',\n",
       "  'arXiv:astro-ph/0608697',\n",
       "  'arXiv:0712.1548']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{list(citations_db.keys())[0]: citations_db[list(citations_db.keys())[0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the set of citations\n",
    "with open(Config.citations_db,'wb') as file:\n",
    "    pickle.dump(citations_db,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now download the articles. To do this we will tweak a bit the script download_to_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute lengths for downloading articles\n",
    "length=0\n",
    "for citations in citations_db.values():\n",
    "    length=length+len(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the script\n",
      "It seems like there was an error in converting 1507.07523.pdf. Please try again later. Exit status 256.           \n",
      "It seems like there was an error in converting 0306581.pdf. Please try again later. Exit status 256.                    \n",
      "It seems like there was an error in converting 1903.0760.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 0306581.pdf. Please try again later. Exit status 256.                     \n",
      "It seems like there was an error in converting 9302006.pdf. Please try again later. Exit status 256.                     \n",
      "It seems like there was an error in converting 1810.00953.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1801.07175.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1705.09887.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1901.07846.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 0709.1616.pdf. Please try again later. Exit status 256.             \n",
      "It seems like there was an error in converting 9310026.pdf. Please try again later. Exit status 256.                   \n",
      "It seems like there was an error in converting 9211042.pdf. Please try again later. Exit status 256.                    \n",
      "It seems like there was an error in converting 9303009.pdf. Please try again later. Exit status 256.                    \n",
      "It seems like there was an error in converting 9211042.pdf. Please try again later. Exit status 256.                    \n",
      "It seems like there was an error in converting 9303009.pdf. Please try again later. Exit status 256.                    \n",
      "It seems like there was an error in converting 9306290.pdf. Please try again later. Exit status 256.                   \n",
      "It seems like there was an error in converting 9408313.pdf. Please try again later. Exit status 256.                   \n",
      "It seems like there was an error in converting 0205006.pdf. Please try again later. Exit status 256.                   \n",
      "It seems like there was an error in converting 9212271.pdf. Please try again later. Exit status 256.                   \n",
      "It seems like there was an error in converting 1710.06353.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1911.0528.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1803.03325.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1602.01017.pdf. Please try again later. Exit status 256.              \n",
      "It seems like there was an error in converting 1109.3855.pdf. Please try again later. Exit status 256.                \n",
      "It seems like there was an error in converting 1803.09941.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1812.00570.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1911.11407.pdf. Please try again later. Exit status 256.                \n",
      "It seems like there was an error in converting 9311013.pdf. Please try again later. Exit status 256.                     \n",
      "It seems like there was an error in converting 9409004.pdf. Please try again later. Exit status 256.                    \n",
      "It seems like there was an error in converting 1201.4648.pdf. Please try again later. Exit status 256.             \n",
      "It seems like there was an error in converting 1907.11699.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 0301008.pdf. Please try again later. Exit status 256.                  \n",
      "It seems like there was an error in converting 1802.04944.pdf. Please try again later. Exit status 256.                \n",
      "It seems like there was an error in converting 1906.07709.pdf. Please try again later. Exit status 256.                  \n",
      "It seems like there was an error in converting 1906.05827.pdf. Please try again later. Exit status 256.            \n",
      "It seems like there was an error in converting 1908.01010.pdf. Please try again later. Exit status 256.            \n",
      "Downloaded 891 articles out of 1513.                                                                               \n"
     ]
    }
   ],
   "source": [
    "def get_id_url(citation):\n",
    "    idx=citation[citation.rfind(':')+1:].split('v')[0]\n",
    "    control=idx.rfind('/')\n",
    "    if control!=-1:\n",
    "        idx=idx[control+1:]\n",
    "    url='http://export.'+citation.replace(':','.org/pdf/')+'.pdf'\n",
    "    return idx,url.lower()\n",
    "\n",
    "if not os.path.exists(Config.tmp): #create directory to temporarily store pdfs if not present aready\n",
    "    os.makedirs(Config.tmp)\n",
    "\n",
    "timeout=10 #waiting seconds before stopping the download\n",
    "already_have = set(os.listdir(Config.txt_db)) #getting list of papers that are already present in the directory  \n",
    "\n",
    "num_to_add=0\n",
    "num_added=0\n",
    "with open('citations_db','rb') as file:\n",
    "    citations_db=pickle.load(file)\n",
    "    \n",
    "print('Starting the script')\n",
    "cycle=0\n",
    "for citations in citations_db.values():\n",
    "    \n",
    "    for citation in citations:\n",
    "        cycle=cycle+1\n",
    "        idx,pdf_url=get_id_url(citation)\n",
    "        txt=idx+'.txt'\n",
    "        pdf=idx+'.pdf'\n",
    "        pdf_path=os.path.join(Config.tmp,pdf)\n",
    "        txt_path=os.path.join(Config.txt_db,txt)\n",
    "        try:\n",
    "            if not txt in already_have:\n",
    "                num_to_add+=1\n",
    "                req = urlopen(pdf_url, None, timeout)\n",
    "                print(('Getting article {}. Progress {:.2f}%'+40*' ').format(pdf_url,100*cycle/length),end='\\r')\n",
    "                with open(pdf_path, 'wb') as file:\n",
    "                    shutil.copyfileobj(req, file)\n",
    "                #converting the pdf into txt needs pdftotext on the system to run\n",
    "                cmd = \"pdftotext %s %s\" % (pdf_path, txt_path)\n",
    "                exit=os.system(cmd)\n",
    "                #remove the pdf to save space\n",
    "                os.system('rm %s'%(pdf_path))\n",
    "                num_added+=1\n",
    "                #check that everything went well\n",
    "                if exit!=0:\n",
    "                    print('It seems like there was an error in converting %s. Please try again later. Exit status %i.'%(pdf,exit))\n",
    "                    #remove the article in case the file was created\n",
    "                    if os.path.isfile(txt_path):\n",
    "                        os.system('rm '+txt_path)\n",
    "                    num_added-=1\n",
    "            \n",
    "            else:\n",
    "                print(('{} already exists, skipping. Progress {:.2f}%'+40*' ').format(idx,100*cycle/length),end='\\r')\n",
    "        except Exception as e:\n",
    "            print(('An error incurred while downloading: %s .'+20*' ')%(pdf_url),end='\\r')\n",
    "            print(str(e)+60*' ',end='\\r')\n",
    "print(('Downloaded %i articles out of %i.'+30*' ')%(num_added,num_to_add))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the strategy to evaluate the model is simple. We create two vectorised datasets, one with the citing articles and one with the cited articles. We then perform a cosine similarty search on the citing articles over the cited articles and we consider how many citations we are able to catch in the first N results. First we therefore need to vectorize the citing and the cited articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not all the articles in citations_db were downloaded, either for problems in the search of citations or because\n",
    "#the link was not valid. so we create a new database with the papers that were actually searched\n",
    "d={}\n",
    "for cited, citations in citations_db.items():\n",
    "    txt_labels=[]\n",
    "    for citation in citations:\n",
    "        idx,dummy=get_id_url(citation)\n",
    "        article_path=os.path.join(Config.txt_db,idx+'.txt')\n",
    "        if os.path.isfile(article_path):\n",
    "            txt_labels.append(idx)\n",
    "    if len(txt_labels)>=10: \n",
    "        d[cited]=txt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1754"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of all cited articles\n",
    "txt_cited=[]\n",
    "for citations in d.values():\n",
    "    txt_cited=txt_cited+citations\n",
    "txt_cited=set(txt_cited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of citations: 29.315279\n",
      "Max number of citations: 501\n",
      "Min number of citations: 10\n"
     ]
    }
   ],
   "source": [
    "lengths=np.array([len(cit) for cit in d.values()])\n",
    "print(\"Mean number of citations: %f\"%lengths.mean())\n",
    "print(\"Max number of citations: %i\"%lengths.max())\n",
    "print(\"Min number of citations: %i\"%lengths.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the cited articles\n",
    "corpus=read_clean(txt_cited)\n",
    "Xctd=tfidf.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cited_vectorized={'Xctd':Xctd,'articles':np.array(list(txt_cited))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorized form of the cited articles\n",
    "with open('cited_vectorized','wb') as file:\n",
    "    pickle.dump(cited_vectorized,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the citing articles \n",
    "corpus=read_clean(d.keys())\n",
    "Xctn=tfidf.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_vectorized={'Xctn':Xctn,'articles':np.array(list(d.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the vectorized form of the citing articles\n",
    "with open('citing_vectorized','wb') as file:\n",
    "    pickle.dump(citing_vectorized,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a function that performs the evaluation. This function simply computes the cosine similarity between the citing articles and the cited articles and finds how many of the cited articles are returned in the first how_many articles. Since for each citing articles the number of cited articles is not always the same, as final resut we will consider the average of the rates of correctly guessed papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a function that evaluate the model\n",
    "def evaluation_reccomender(cited_vectorized,citing_vectorized,d):\n",
    "    Xctd=cited_vectorized['Xctd']\n",
    "    articles_ctd=cited_vectorized['articles']\n",
    "    Xctn=citing_vectorized['Xctn']\n",
    "    articles_ctn=citing_vectorized['articles']\n",
    "    similar_articles=find_similar(Xctd,Xctn,Xctd.shape[0])\n",
    "    n_similars=0\n",
    "    for i,similar in enumerate(similar_articles):\n",
    "    #For each citing article we decide to take as the number of articles to consider the number of cited articles\n",
    "        how_many=len(d[articles_ctn[i]])\n",
    "        n_similar=len(set(articles_ctd[similar[:how_many]]).intersection(set(d[articles_ctn[i]])))\n",
    "        n_similars=n_similars+n_similar/how_many\n",
    "    #Take the mean and return \n",
    "    return n_similars/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of correctly guessed papers is 0.397\n"
     ]
    }
   ],
   "source": [
    "print(\"The ratio of correctly guessed papers is {:.3f}\".format(evaluation_reccomender(cited_vectorized,citing_vectorized,d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the fraction of correctly guessed articles is approximately 40%. This result is definitly not exciting, but we need to consider that only a fraction of the cited papers can be found on the arXiv. It would be interesting to consider what results we would get with more available resources. A possible improvement of the reccomender system would be to use an approach similar to the collaborative filtering, but also in this case we would need access to more resources. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
